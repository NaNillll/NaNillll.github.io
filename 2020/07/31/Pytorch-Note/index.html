<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Note,Pytorch," />










<meta name="description" content="AUTOGRAD SEMANTICShttps:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;1.1.0&#x2F;notes&#x2F;autograd.html Excluding subgraphs from backward任何Tensor都有一个bool变量requires_grad，用于梯度计算时选择是否排除相关子图 只有一个操作的所有输入Tensor均不需求梯度，输出的Tensor才不需要求梯度 新建的Tens">
<meta property="og:type" content="article">
<meta property="og:title" content="Notes-of-Pytorch-Notes">
<meta property="og:url" content="http://yoursite.com/2020/07/31/Pytorch-Note/index.html">
<meta property="og:site_name" content="NaNillll&#39;s Blog">
<meta property="og:description" content="AUTOGRAD SEMANTICShttps:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;1.1.0&#x2F;notes&#x2F;autograd.html Excluding subgraphs from backward任何Tensor都有一个bool变量requires_grad，用于梯度计算时选择是否排除相关子图 只有一个操作的所有输入Tensor均不需求梯度，输出的Tensor才不需要求梯度 新建的Tens">
<meta property="article:published_time" content="2020-07-31T10:20:19.000Z">
<meta property="article:modified_time" content="2020-08-05T01:22:39.009Z">
<meta property="article:author" content="NaNillll">
<meta property="article:tag" content="Note">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/07/31/Pytorch-Note/"/>





  <title>Notes-of-Pytorch-Notes | NaNillll's Blog</title>
  








<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">NaNillll's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/31/Pytorch-Note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="NaNillll">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NaNillll's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Notes-of-Pytorch-Notes</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-31T18:20:19+08:00">
                2020-07-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3.3k
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="AUTOGRAD-SEMANTICS"><a href="#AUTOGRAD-SEMANTICS" class="headerlink" title="AUTOGRAD SEMANTICS"></a>AUTOGRAD SEMANTICS</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/autograd.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/autograd.html</a></p>
<h3 id="Excluding-subgraphs-from-backward"><a href="#Excluding-subgraphs-from-backward" class="headerlink" title="Excluding subgraphs from backward"></a>Excluding subgraphs from backward</h3><p>任何Tensor都有一个bool变量requires_grad，用于梯度计算时选择是否排除相关子图</p>
<p>只有一个操作的所有输入Tensor均不需求梯度，输出的Tensor才不需要求梯度</p>
<p>新建的Tensor默认不需求梯度，而新建的nn模块一般来说默认需要</p>
<p>可以通过这个机制只训练网络的某一部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment"># Replace the last fully-connected layer</span></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></span><br><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimize only the classifier</span></span><br><span class="line">optimizer = optim.SGD(model.fc.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h3 id="How-autograd-encodes-the-history"><a href="#How-autograd-encodes-the-history" class="headerlink" title="How autograd encodes the history"></a>How autograd encodes the history</h3><p>autograd 记录了一个计算图，包含了所有产生新数据的操作，这是一个有向无环图，叶节点是输入Tensor，根节点是输出Tensor。利用这个图可以很便利的计算梯度。</p>
<p>autograd 将这个图保存为包含autograd.Function类的图，在前向传播时建立起这个图，Tensor的.grad_fn作为entry指向这个图，反向传播时使用该图进行梯度计算</p>
<p>每一个iteration都会从头重新自动生成计算图</p>
<h3 id="In-place-operations-with-autograd"><a href="#In-place-operations-with-autograd" class="headerlink" title="In-place operations with autograd"></a>In-place operations with autograd</h3><p>in-place operation在指改变一个tensor的值的时候，不经过复制操作，直接在原内存上改变它的值。对此类操作的autograd 是比较困难的，而且这类操作并不会很好的节约内存，所以请尽量不要使用。减少in-place操作的两个理由：</p>
<p>1.可能会overwrite需要求导的变量</p>
<p>2.Out-of-place的操作只需在计算图中新建变量，而每次inplace操作都需要修改该操作所有输入的创建者，重写计算图</p>
<p>为了保证inplace操作的正确性，每一个tensor保存了一个version  counter，记录被修改的次数。当Function保存tensor用于反向传播时，会保存所有输入tensor的version counter，访问self.saved_tensors后，将对其进行检查，如果该值大于保存的值，则会报错。</p>
<h2 id="BROADCASTING-SEMANTICS"><a href="#BROADCASTING-SEMANTICS" class="headerlink" title="BROADCASTING  SEMANTICS"></a>BROADCASTING  SEMANTICS</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/broadcasting.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/broadcasting.html</a></p>
<p>喜闻乐见的广播机制</p>
<p>两个tensor可以广播，必须满足：</p>
<p>1.每个tensor至少一维</p>
<p>2.从最后一维开始，两tensor的维数必须相同或其中一个为1或其中一个不存在</p>
<p>如果两个tensor可以广播，则结果的尺寸：</p>
<p>1.如果维数不等，则在较少的tensor前置1维直到相等</p>
<p>2.结果的每一维的维数，是两个tensor中维数更多的</p>
<p>对于inplace操作，inplace tensor不能被改变尺寸</p>
<h3 id="Backwards-compatibility"><a href="#Backwards-compatibility" class="headerlink" title="Backwards compatibility"></a>Backwards compatibility</h3><p>在之前的版本中，只要两个tensor元素数量相同，就能都自动view成一维然后pointwise 操作。现在的Pytorch支持了广播，因此现在如果两个tensor不能广播但元素数量相同，pointwise 会警告</p>
<p>同样，引入这样的广播机制可能在向后传播时产生不兼容的情况，为了确定这种情况，可以将torch.utils.backcompat.broadcast_warning.enabled设置为True，来生成相关的警告</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.utils.backcompat.broadcast_warning.enabled=<span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.add(torch.ones(<span class="number">4</span>,<span class="number">1</span>), torch.ones(<span class="number">4</span>))</span><br><span class="line">__main__:<span class="number">1</span>: UserWarning: self <span class="keyword">and</span> other do <span class="keyword">not</span> have the same shape, but are broadcastable, <span class="keyword">and</span> have the same number of elements.</span><br><span class="line">Changing behavior <span class="keyword">in</span> a backwards incompatible manner to broadcasting rather than viewing <span class="keyword">as</span> <span class="number">1</span>-dimensional.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>常见的某积：</p>
<p><a href="https://www.cnblogs.com/steven-yang/p/6348112.html" target="_blank" rel="noopener">https://www.cnblogs.com/steven-yang/p/6348112.html</a></p>
<p><a href="https://zh.wikipedia.org/wiki/外積_(消歧義" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E5%A4%96%E7%A9%8D_(%E6%B6%88%E6%AD%A7%E7%BE%A9)</a>)</p>
</blockquote>
<h2 id="CUDA-SEMANTICS"><a href="#CUDA-SEMANTICS" class="headerlink" title="CUDA SEMANTICS"></a>CUDA SEMANTICS</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/cuda.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/cuda.html</a></p>
<p>torch.cuda类用于进行cuda操作，记录当前默认的GPU。可以使用torch.cuda.device类进行修改</p>
<p>但是一旦某个tensor的cuda确定了，由这个tensor生成的tensor也在同一设备</p>
<p>默认情况下，不允许跨GPU操作，除了copy_()和其他具有类似复制功能的操作，比如to()或cuda()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cuda = torch.device(<span class="string">'cuda'</span>)     <span class="comment"># Default CUDA device</span></span><br><span class="line">cuda0 = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">cuda2 = torch.device(<span class="string">'cuda:2'</span>)  <span class="comment"># GPU 2 (these are 0-indexed)</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>], device=cuda0)</span><br><span class="line"><span class="comment"># x.device is device(type='cuda', index=0)</span></span><br><span class="line">y = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).cuda()</span><br><span class="line"><span class="comment"># y.device is device(type='cuda', index=0)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.cuda.device(<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># allocates a tensor on GPU 1</span></span><br><span class="line">    a = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>], device=cuda)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># transfers a tensor from CPU to GPU 1</span></span><br><span class="line">    b = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).cuda()</span><br><span class="line">    <span class="comment"># a.device and b.device are device(type='cuda', index=1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># You can also use ``Tensor.to`` to transfer a tensor:</span></span><br><span class="line">    b2 = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).to(device=cuda)</span><br><span class="line">    <span class="comment"># b.device and b2.device are device(type='cuda', index=1)</span></span><br><span class="line"></span><br><span class="line">    c = a + b</span><br><span class="line">    <span class="comment"># c.device is device(type='cuda', index=1)</span></span><br><span class="line"></span><br><span class="line">    z = x + y</span><br><span class="line">    <span class="comment"># z.device is device(type='cuda', index=0)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># even within a context, you can specify the device</span></span><br><span class="line">    <span class="comment"># (or give a GPU index to the .cuda call)</span></span><br><span class="line">    d = torch.randn(<span class="number">2</span>, device=cuda2)</span><br><span class="line">    e = torch.randn(<span class="number">2</span>).to(cuda2)</span><br><span class="line">    f = torch.randn(<span class="number">2</span>).cuda(cuda2)</span><br><span class="line">    <span class="comment"># d.device, e.device, and f.device are all device(type='cuda', index=2)</span></span><br></pre></td></tr></table></figure>
<h3 id="异步操作"><a href="#异步操作" class="headerlink" title="异步操作"></a>异步操作</h3><p>通常情况下，GPU操作异步。显然，异步操作有很多好处</p>
<p>如果要强制同步计算，设置环境变量CUDA_LAUNCH_BLOCKING=1，便于debug时寻找错误</p>
<p>一些诸如to()和copy()的函数可以显示设置non_blocking来绕过同步</p>
<h3 id="CUDA-streams"><a href="#CUDA-streams" class="headerlink" title="CUDA streams"></a>CUDA streams</h3><p>cuda流是每个设备的线性执行序列。不同设备间操作并行，而同一cuda流中操作按序</p>
<p>pytorch会自动的对默认流进行调度，但是对于用户自己创建的流就需要用户自行调度了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cuda = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">s = torch.cuda.Stream()  <span class="comment"># Create a new stream.</span></span><br><span class="line">A = torch.empty((<span class="number">100</span>, <span class="number">100</span>), device=cuda).normal_(<span class="number">0.0</span>, <span class="number">1.0</span>)</span><br><span class="line"><span class="keyword">with</span> torch.cuda.stream(s):</span><br><span class="line">    <span class="comment"># sum() may start execution before normal_() finishes!</span></span><br><span class="line">    B = torch.sum(A)</span><br></pre></td></tr></table></figure>
<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><p>PyTorch使用缓存分配器来加速内存分配，无需设备同步即可快速释放内存。 但是，如果使用nvidia-smi，分配器管理的未使用内存仍将显示。可以使用empty_cache来释放这些未使用的缓存，但是这样并不增加Pytorch可用的GPU内存</p>
<h3 id="Best-practices"><a href="#Best-practices" class="headerlink" title="Best practices"></a>Best practices</h3><p>首先，确定是否使用GPU，一般使用cuda.is_available()判断，然后设置</p>
<p>在使用多GPU时，设置CUDA_VISIBLE_DEVICES来确定可用的GPU。手动设置默认GPU可以使用torch.cuda.device()</p>
<p>torch.Tensor.new_<em>类和 </em> _like类函数创建新tensor时会保存包括cuda属性在内的大部分属性，这是一种在前向传播过程中新建变量比较好的方法</p>
<p>源于pinned (page-locked) memory的数据从主机搬运到GPU上会更快，可以使用pin_memoy()生成cpu tensor得到副本</p>
<p>此外，一旦固定张量或存储，就可以使用异步GPU副本。只需将一个额外的non_blocking = True参数传递给cuda()调用即可。这可用于重叠进行数据传输与计算。</p>
<p>在创建DataLoader时可以设置pin_memory = True</p>
<p>可以使用nn.DataParallel 代替multiprocessing进行多进程操作，后者经常会出现很多问题</p>
<h2 id="EXTENDING-PYTORCH"><a href="#EXTENDING-PYTORCH" class="headerlink" title="EXTENDING PYTORCH"></a>EXTENDING PYTORCH</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/extending.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/extending.html</a></p>
<h3 id="扩展torch-autograd"><a href="#扩展torch-autograd" class="headerlink" title="扩展torch.autograd"></a>扩展torch.autograd</h3><p>tensor上的每一个操作都会创建一个新的function 对象，执行计算并记录进行的情况。计算的历史保存为DAG，顶点为function，边为计算关系（input &lt;- output），反向传播用拓扑排序的顺序来传递梯度</p>
<p>每一个function的object都只能在前向传播中使用一次</p>
<p>扩展torch.autograd等同于添加orch.autograd.Function的一个子类。任何torch.autograd的新函数都需要实现forward()和backward()</p>
<ul>
<li>forward：进行该function操作。可以输入任意多任何类型的参数，其中有些还可以作为optional的，第一个参数必须是用于保存上下文的ctx，在反向传播中被用于获得function的参数。记录历史的（即requires_grad = True）的Tensor参数将在调用之前不跟踪历史，且将会加入到计算图中。返回值可以是单个tensor或tensor的tuple</li>
<li>backward：对该function求导。接受上文的ctx作为第一个参数，接着跟随和forward返回的数量相同的参数。返回值数量应该和forward输入值相同。ctx可用于获取前向传播中保存的tensor参数。ctx也有一个tuple 变量ctx.needs_input_grad用于控制每一个input是否需要求梯度。每一传入的参数都是该节点关于给定output的梯度，而返回值应该是关于相应input的梯度，如果该input不是tensor类或者不需要求导，可以返回None</li>
</ul>
<p>torch.nn类中线性函数的实例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inherit from Function</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearFunction</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Note that both forward and backward are @staticmethods</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="comment"># bias is an optional argument</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input, weight, bias=None)</span>:</span></span><br><span class="line">        ctx.save_for_backward(input, weight, bias)</span><br><span class="line">        output = input.mm(weight.t())</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output += bias.unsqueeze(<span class="number">0</span>).expand_as(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This function has only a single output, so it gets only one gradient</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="comment"># This is a pattern that is very convenient - at the top of backward</span></span><br><span class="line">        <span class="comment"># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span></span><br><span class="line">        <span class="comment"># None. Thanks to the fact that additional trailing Nones are</span></span><br><span class="line">        <span class="comment"># ignored, the return statement is simple even when the function has</span></span><br><span class="line">        <span class="comment"># optional inputs.</span></span><br><span class="line">        input, weight, bias = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_weight = grad_bias = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># These needs_input_grad checks are optional and there only to</span></span><br><span class="line">        <span class="comment"># improve efficiency. If you want to make your code simpler, you can</span></span><br><span class="line">        <span class="comment"># skip them. Returning gradients for inputs that don't require it is</span></span><br><span class="line">        <span class="comment"># not an error.</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">0</span>]:</span><br><span class="line">            grad_input = grad_output.mm(weight)</span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            grad_weight = grad_output.t().mm(input)</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> ctx.needs_input_grad[<span class="number">2</span>]:</span><br><span class="line">            grad_bias = grad_output.sum(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grad_input, grad_weight, grad_bias</span><br></pre></td></tr></table></figure>
<p>为了利用该函数，将他的apply形式起别名:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear = LinearFunction.apply</span><br></pre></td></tr></table></figure>
<p>当然，也可以使用none-Tensor参数来传参：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MulConstant</span><span class="params">(Function)</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, tensor, constant)</span>:</span></span><br><span class="line">        <span class="comment"># ctx is a context object that can be used to stash information</span></span><br><span class="line">        <span class="comment"># for backward computation</span></span><br><span class="line">        ctx.constant = constant</span><br><span class="line">        <span class="keyword">return</span> tensor * constant</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="comment"># We return as many input gradients as there were arguments.</span></span><br><span class="line">        <span class="comment"># Gradients of non-Tensor arguments to forward must be None.</span></span><br><span class="line">        <span class="keyword">return</span> grad_output * ctx.constant, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>backward的输入也可以是记录历史的tensor，因此可以在backward中实现高阶微分操作</p>
<p>如果要对自己编写的模块进行梯度校验，可以使用torch.autograd.gradcheck模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> gradcheck</span><br><span class="line"></span><br><span class="line"><span class="comment"># gradcheck takes a tuple of tensors as input, check if your gradient</span></span><br><span class="line"><span class="comment"># evaluated with these tensors are close enough to numerical</span></span><br><span class="line"><span class="comment"># approximations and returns True if they all verify this condition.</span></span><br><span class="line">input = (torch.randn(<span class="number">20</span>,<span class="number">20</span>,dtype=torch.double,requires_grad=<span class="literal">True</span>), torch.randn(<span class="number">30</span>,<span class="number">20</span>,dtype=torch.double,requires_grad=<span class="literal">True</span>))</span><br><span class="line">test = gradcheck(linear, input, eps=<span class="number">1e-6</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line">print(test)</span><br></pre></td></tr></table></figure>
<h3 id="扩展torch-nn"><a href="#扩展torch-nn" class="headerlink" title="扩展torch.nn"></a>扩展torch.nn</h3><p>nn导出两种接口，modules versions and functional versions</p>
<h3 id="添加新的Module"><a href="#添加新的Module" class="headerlink" title="添加新的Module"></a>添加新的Module</h3><p>由于nn大量利用了autograd，因此添加新模块需要实现一个进行操作和求梯度的Function</p>
<p>要想将Function扩充为一个module，需要实现：</p>
<p><strong> init</strong>：可选，输入相关参数，初始化parameter和buffer</p>
<p>forward：实例化function并用它进行操作计算</p>
<blockquote>
<p>什么是buffer：</p>
<p><a href="https://zhuanlan.zhihu.com/p/89442276" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/89442276</a></p>
<p><a href="https://stackoverflow.com/questions/57540745/what-is-the-difference-between-register-parameter-and-register-buffer-in-pytorch" target="_blank" rel="noopener">https://stackoverflow.com/questions/57540745/what-is-the-difference-between-register-parameter-and-register-buffer-in-pytorch</a></p>
</blockquote>
<p>一个Linear模块的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_features, output_features, bias=True)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()</span><br><span class="line">        self.input_features = input_features</span><br><span class="line">        self.output_features = output_features</span><br><span class="line"></span><br><span class="line">        <span class="comment"># nn.Parameter is a special kind of Tensor, that will get</span></span><br><span class="line">        <span class="comment"># automatically registered as Module's parameter once it's assigned</span></span><br><span class="line">        <span class="comment"># as an attribute. Parameters and buffers need to be registered, or</span></span><br><span class="line">        <span class="comment"># they won't appear in .parameters() (doesn't apply to buffers), and</span></span><br><span class="line">        <span class="comment"># won't be converted when e.g. .cuda() is called. You can use</span></span><br><span class="line">        <span class="comment"># .register_buffer() to register buffers.</span></span><br><span class="line">        <span class="comment"># nn.Parameters require gradients by default.</span></span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(output_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># You should always register all possible parameters, but the</span></span><br><span class="line">            <span class="comment"># optional ones can be None if you want.</span></span><br><span class="line">            self.register_parameter(<span class="string">'bias'</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Not a very smart way to initialize weights</span></span><br><span class="line">        self.weight.data.uniform_(<span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.bias.data.uniform_(<span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="comment"># See the autograd section for explanation of what happens here.</span></span><br><span class="line">        <span class="keyword">return</span> LinearFunction.apply(input, self.weight, self.bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extra_repr</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># (Optional)Set the extra information about this module. You can test</span></span><br><span class="line">        <span class="comment"># it by printing an object of this class.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'in_features=&#123;&#125;, out_features=&#123;&#125;, bias=&#123;&#125;'</span>.format(</span><br><span class="line">            self.in_features, self.out_features, self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h2 id="节约GPU的内存"><a href="#节约GPU的内存" class="headerlink" title="节约GPU的内存"></a>节约GPU的内存</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/faq.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/faq.html</a></p>
<h3 id="My-model-reports-“cuda-runtime-error-2-out-of-memory”"><a href="#My-model-reports-“cuda-runtime-error-2-out-of-memory”" class="headerlink" title="My model reports “cuda runtime error(2): out of memory”"></a>My model reports “cuda runtime error(2): out of memory”</h3><p>用完了GPU的内存，常见解决方法如下：</p>
<p>1.不要积累训练循环的历史</p>
<p>一般来说，包含记录梯度的tensor的计算都需要保存计算历史，不要让这样的tensor参与存在周期大于循环的计算中，比如累计计数。相反，应该使用detach或者直接访问基础数据</p>
<p>参考下面的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(input)</span><br><span class="line">    loss = criterion(output)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    total_loss += loss</span><br></pre></td></tr></table></figure>
<p>在这里，total_loss正在累积整个训练循环的历史记录，因为loss是一个保存求梯度历史的变量</p>
<p>要解决问题，只需要修改为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_loss += float(loss)</span><br></pre></td></tr></table></figure>
<p>2.不需要的tensor和变量赶快释放</p>
<p>对于Python，如果分配某个tensor给本地或者成员变量，直到超出这个变量的作用范围，才会释放最初的tensor</p>
<p>参考下面的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    intermediate = f(input[i])</span><br><span class="line">    result += g(intermediate)</span><br><span class="line">output = h(result)</span><br><span class="line"><span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>在这里，即使h正在执行，intermediate也会被保存，因为它的作用域超出了循环的结尾。为了显示释放空间，可以使用del intermediate</p>
<p>3.不要在太大的序列上跑RNN</p>
<p>4.不要使用太大的linear layers</p>
<h2 id="SERIALIZATION-SEMANTICS"><a href="#SERIALIZATION-SEMANTICS" class="headerlink" title="SERIALIZATION SEMANTICS"></a>SERIALIZATION SEMANTICS</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/serialization.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/serialization.html</a></p>
<h3 id="保存模型的方法"><a href="#保存模型的方法" class="headerlink" title="保存模型的方法"></a>保存模型的方法</h3><p>1.只保存模型参数（推荐）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.save(the_model.state_dict(), PATH)</span><br><span class="line"></span><br><span class="line">the_model = TheModelClass(*args, **kwargs)</span><br><span class="line">the_model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<p>2.保存整个模型</p>
<p>在这种情况下，序列化的数据将绑定到特定类和确切的目录，因此在其他project中使用时或重构后，可能会出现很多报错</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.save(the_model, PATH)</span><br><span class="line"></span><br><span class="line">the_model = torch.load(PATH)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.jianshu.com/p/60fc57e19615" target="_blank" rel="noopener">https://www.jianshu.com/p/60fc57e19615</a></p>
</blockquote>
<h2 id="多进程实例"><a href="#多进程实例" class="headerlink" title="多进程实例"></a>多进程实例</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/multiprocessing.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/multiprocessing.html</a></p>
<p>torch.multiprocessing用来替代Python本身的mutiprocessing</p>
<h2 id="设置随机数种子便于复现"><a href="#设置随机数种子便于复现" class="headerlink" title="设置随机数种子便于复现"></a>设置随机数种子便于复现</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/randomness.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/randomness.html</a></p>
<p>网络的参数最初是随机的</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Note/" rel="tag"><i class="fa fa-tag"></i> Note</a>
          
            <a href="/tags/Pytorch/" rel="tag"><i class="fa fa-tag"></i> Pytorch</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/07/14/LBS/" rel="next" title="LBS">
                <i class="fa fa-chevron-left"></i> LBS
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/08/05/Pytorch-recipes/" rel="prev" title="Pytorch-recipes">
                Pytorch-recipes <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">NaNillll</p>
              <p class="site-description motion-element" itemprop="description">记忆是有限的，外部记忆体是无限的</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/NaNillll" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#AUTOGRAD-SEMANTICS"><span class="nav-number">1.</span> <span class="nav-text">AUTOGRAD SEMANTICS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Excluding-subgraphs-from-backward"><span class="nav-number">1.1.</span> <span class="nav-text">Excluding subgraphs from backward</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-autograd-encodes-the-history"><span class="nav-number">1.2.</span> <span class="nav-text">How autograd encodes the history</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#In-place-operations-with-autograd"><span class="nav-number">1.3.</span> <span class="nav-text">In-place operations with autograd</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BROADCASTING-SEMANTICS"><span class="nav-number">2.</span> <span class="nav-text">BROADCASTING  SEMANTICS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Backwards-compatibility"><span class="nav-number">2.1.</span> <span class="nav-text">Backwards compatibility</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CUDA-SEMANTICS"><span class="nav-number">3.</span> <span class="nav-text">CUDA SEMANTICS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#异步操作"><span class="nav-number">3.1.</span> <span class="nav-text">异步操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CUDA-streams"><span class="nav-number">3.2.</span> <span class="nav-text">CUDA streams</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#内存管理"><span class="nav-number">3.3.</span> <span class="nav-text">内存管理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Best-practices"><span class="nav-number">3.4.</span> <span class="nav-text">Best practices</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EXTENDING-PYTORCH"><span class="nav-number">4.</span> <span class="nav-text">EXTENDING PYTORCH</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#扩展torch-autograd"><span class="nav-number">4.1.</span> <span class="nav-text">扩展torch.autograd</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#扩展torch-nn"><span class="nav-number">4.2.</span> <span class="nav-text">扩展torch.nn</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#添加新的Module"><span class="nav-number">4.3.</span> <span class="nav-text">添加新的Module</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#节约GPU的内存"><span class="nav-number">5.</span> <span class="nav-text">节约GPU的内存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#My-model-reports-“cuda-runtime-error-2-out-of-memory”"><span class="nav-number">5.1.</span> <span class="nav-text">My model reports “cuda runtime error(2): out of memory”</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SERIALIZATION-SEMANTICS"><span class="nav-number">6.</span> <span class="nav-text">SERIALIZATION SEMANTICS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#保存模型的方法"><span class="nav-number">6.1.</span> <span class="nav-text">保存模型的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多进程实例"><span class="nav-number">7.</span> <span class="nav-text">多进程实例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#设置随机数种子便于复现"><span class="nav-number">8.</span> <span class="nav-text">设置随机数种子便于复现</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">NaNillll</span>
  
 
  
  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">5.8k</span>
  
</div>











        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  

</body>
</html>
