<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="记忆是有限的，外部记忆体是无限的">
<meta property="og:type" content="website">
<meta property="og:title" content="NaNillll&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="NaNillll&#39;s Blog">
<meta property="og:description" content="记忆是有限的，外部记忆体是无限的">
<meta property="article:author" content="NaNillll">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>NaNillll's Blog</title>
  








<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">NaNillll's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/05/Pytorch-recipes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="NaNillll">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NaNillll's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/08/05/Pytorch-recipes/" itemprop="url">笔记Pytorch-Recipes</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-08-05T09:15:12+08:00">
                2020-08-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  254
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>Dataset：含有<strong> len </strong> () 与 <strong> getitem</strong>()函数，Python len()函数可以得到前者</p>
<p>TensorDataset：包装x与y，便于选择</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_ds = TensorDataset(x_train, y_train)</span><br><span class="line">xb,yb = train_ds[i*bs : i*bs+bs]</span><br></pre></td></tr></table></figure>
<p>DataLoader：方便选择与迭代minibatch的数据</p>
<h3 id="model-eval-amp-model-train"><a href="#model-eval-amp-model-train" class="headerlink" title="model.eval()&amp;model.train()"></a>model.eval()&amp;model.train()</h3><p>用来保证不同阶段中dropout和BatchNorm的等操作的正确进行</p>
<p><a href="https://blog.csdn.net/songyunli1111/article/details/89071021" target="_blank" rel="noopener">https://blog.csdn.net/songyunli1111/article/details/89071021</a></p>
<p><a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/8724433.html</a></p>
<h3 id="nn-Sequential"><a href="#nn-Sequential" class="headerlink" title="nn.Sequential"></a>nn.Sequential</h3><p>包装多个依次进行的操作</p>
<p>可以在Sequential中添加自定义函数，需要继承nn.Module</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lambda</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, func)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.func = func</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.func(x)</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> x.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    Lambda(preprocess),</span><br><span class="line">    Lambda(<span class="keyword">lambda</span> x: x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="state-dict"><a href="#state-dict" class="headerlink" title="state_dict"></a>state_dict</h3><p>state_dict本质是Python的dict类，包含了每个类的参数</p>
<p>只有具有可学习参数或者registered buffers的layer才有state_dict。Optimizer 也有state_dict</p>
<p>可以用torch.save保存</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Specify a path</span></span><br><span class="line">PATH = <span class="string">"state_dict_model.pt"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Save</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load</span></span><br><span class="line">model = Net()</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure>
<p>通常保存为.pt/.pth文件</p>
<p>如果接下来需要evaluate，不要忘记调用eval</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/31/Pytorch-Note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="NaNillll">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NaNillll's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/07/31/Pytorch-Note/" itemprop="url">笔记Pytorch-Notes</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-31T18:20:19+08:00">
                2020-07-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  3.3k
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><em><del>约等于直接翻译</del></em></p>
<h2 id="AUTOGRAD-SEMANTICS"><a href="#AUTOGRAD-SEMANTICS" class="headerlink" title="AUTOGRAD SEMANTICS"></a>AUTOGRAD SEMANTICS</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/autograd.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/autograd.html</a></p>
<h3 id="Excluding-subgraphs-from-backward"><a href="#Excluding-subgraphs-from-backward" class="headerlink" title="Excluding subgraphs from backward"></a>Excluding subgraphs from backward</h3><p>任何Tensor都有一个bool变量requires_grad，用于梯度计算时选择是否排除相关子图</p>
<p>只有一个操作的所有输入Tensor均不需求梯度，输出的Tensor才不需要求梯度</p>
<p>新建的Tensor默认不需求梯度，而新建的nn模块一般来说默认需要</p>
<p>可以通过这个机制只训练网络的某一部分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment"># Replace the last fully-connected layer</span></span><br><span class="line"><span class="comment"># Parameters of newly constructed modules have requires_grad=True by default</span></span><br><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimize only the classifier</span></span><br><span class="line">optimizer = optim.SGD(model.fc.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h3 id="How-autograd-encodes-the-history"><a href="#How-autograd-encodes-the-history" class="headerlink" title="How autograd encodes the history"></a>How autograd encodes the history</h3><p>autograd 记录了一个计算图，包含了所有产生新数据的操作，这是一个有向无环图，叶节点是输入Tensor，根节点是输出Tensor。利用这个图可以很便利的计算梯度。</p>
<p>autograd 将这个图保存为包含autograd.Function类的图，在前向传播时建立起这个图，Tensor的.grad_fn作为entry指向这个图，反向传播时使用该图进行梯度计算</p>
<p>每一个iteration都会从头重新自动生成计算图</p>
<h3 id="In-place-operations-with-autograd"><a href="#In-place-operations-with-autograd" class="headerlink" title="In-place operations with autograd"></a>In-place operations with autograd</h3><p>in-place operation在指改变一个tensor的值的时候，不经过复制操作，直接在原内存上改变它的值。对此类操作的autograd 是比较困难的，而且这类操作并不会很好的节约内存，所以请尽量不要使用。减少in-place操作的两个理由：</p>
<p>1.可能会overwrite需要求导的变量</p>
<p>2.Out-of-place的操作只需在计算图中新建变量，而每次inplace操作都需要修改该操作所有输入的创建者，重写计算图</p>
<p>为了保证inplace操作的正确性，每一个tensor保存了一个version  counter，记录被修改的次数。当Function保存tensor用于反向传播时，会保存所有输入tensor的version counter，访问self.saved_tensors后，将对其进行检查，如果该值大于保存的值，则会报错。</p>
<h2 id="BROADCASTING-SEMANTICS"><a href="#BROADCASTING-SEMANTICS" class="headerlink" title="BROADCASTING  SEMANTICS"></a>BROADCASTING  SEMANTICS</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/broadcasting.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/broadcasting.html</a></p>
<p>喜闻乐见的广播机制</p>
<p>两个tensor可以广播，必须满足：</p>
<p>1.每个tensor至少一维</p>
<p>2.从最后一维开始，两tensor的维数必须相同或其中一个为1或其中一个不存在</p>
<p>如果两个tensor可以广播，则结果的尺寸：</p>
<p>1.如果维数不等，则在较少的tensor前置1维直到相等</p>
<p>2.结果的每一维的维数，是两个tensor中维数更多的</p>
<p>对于inplace操作，inplace tensor不能被改变尺寸</p>
<h3 id="Backwards-compatibility"><a href="#Backwards-compatibility" class="headerlink" title="Backwards compatibility"></a>Backwards compatibility</h3><p>在之前的版本中，只要两个tensor元素数量相同，就能都自动view成一维然后pointwise 操作。现在的Pytorch支持了广播，因此现在如果两个tensor不能广播但元素数量相同，pointwise 会警告</p>
<p>同样，引入这样的广播机制可能在向后传播时产生不兼容的情况，为了确定这种情况，可以将torch.utils.backcompat.broadcast_warning.enabled设置为True，来生成相关的警告</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.utils.backcompat.broadcast_warning.enabled=<span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.add(torch.ones(<span class="number">4</span>,<span class="number">1</span>), torch.ones(<span class="number">4</span>))</span><br><span class="line">__main__:<span class="number">1</span>: UserWarning: self <span class="keyword">and</span> other do <span class="keyword">not</span> have the same shape, but are broadcastable, <span class="keyword">and</span> have the same number of elements.</span><br><span class="line">Changing behavior <span class="keyword">in</span> a backwards incompatible manner to broadcasting rather than viewing <span class="keyword">as</span> <span class="number">1</span>-dimensional.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>常见的某积：</p>
<p><a href="https://www.cnblogs.com/steven-yang/p/6348112.html" target="_blank" rel="noopener">https://www.cnblogs.com/steven-yang/p/6348112.html</a></p>
<p><a href="https://zh.wikipedia.org/wiki/外積_(消歧義" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E5%A4%96%E7%A9%8D_(%E6%B6%88%E6%AD%A7%E7%BE%A9)</a>)</p>
</blockquote>
<h2 id="CUDA-SEMANTICS"><a href="#CUDA-SEMANTICS" class="headerlink" title="CUDA SEMANTICS"></a>CUDA SEMANTICS</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/cuda.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/cuda.html</a></p>
<p>torch.cuda类用于进行cuda操作，记录当前默认的GPU。可以使用torch.cuda.device类进行修改</p>
<p>但是一旦某个tensor的cuda确定了，由这个tensor生成的tensor也在同一设备</p>
<p>默认情况下，不允许跨GPU操作，除了copy_()和其他具有类似复制功能的操作，比如to()或cuda()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cuda = torch.device(<span class="string">'cuda'</span>)     <span class="comment"># Default CUDA device</span></span><br><span class="line">cuda0 = torch.device(<span class="string">'cuda:0'</span>)</span><br><span class="line">cuda2 = torch.device(<span class="string">'cuda:2'</span>)  <span class="comment"># GPU 2 (these are 0-indexed)</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>], device=cuda0)</span><br><span class="line"><span class="comment"># x.device is device(type='cuda', index=0)</span></span><br><span class="line">y = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).cuda()</span><br><span class="line"><span class="comment"># y.device is device(type='cuda', index=0)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.cuda.device(<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># allocates a tensor on GPU 1</span></span><br><span class="line">    a = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>], device=cuda)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># transfers a tensor from CPU to GPU 1</span></span><br><span class="line">    b = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).cuda()</span><br><span class="line">    <span class="comment"># a.device and b.device are device(type='cuda', index=1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># You can also use ``Tensor.to`` to transfer a tensor:</span></span><br><span class="line">    b2 = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).to(device=cuda)</span><br><span class="line">    <span class="comment"># b.device and b2.device are device(type='cuda', index=1)</span></span><br><span class="line"></span><br><span class="line">    c = a + b</span><br><span class="line">    <span class="comment"># c.device is device(type='cuda', index=1)</span></span><br><span class="line"></span><br><span class="line">    z = x + y</span><br><span class="line">    <span class="comment"># z.device is device(type='cuda', index=0)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># even within a context, you can specify the device</span></span><br><span class="line">    <span class="comment"># (or give a GPU index to the .cuda call)</span></span><br><span class="line">    d = torch.randn(<span class="number">2</span>, device=cuda2)</span><br><span class="line">    e = torch.randn(<span class="number">2</span>).to(cuda2)</span><br><span class="line">    f = torch.randn(<span class="number">2</span>).cuda(cuda2)</span><br><span class="line">    <span class="comment"># d.device, e.device, and f.device are all device(type='cuda', index=2)</span></span><br></pre></td></tr></table></figure>
<h3 id="异步操作"><a href="#异步操作" class="headerlink" title="异步操作"></a>异步操作</h3><p>通常情况下，GPU操作异步。显然，异步操作有很多好处</p>
<p>如果要强制同步计算，设置环境变量CUDA_LAUNCH_BLOCKING=1，便于debug时寻找错误</p>
<p>一些诸如to()和copy()的函数可以显示设置non_blocking来绕过同步</p>
<h3 id="CUDA-streams"><a href="#CUDA-streams" class="headerlink" title="CUDA streams"></a>CUDA streams</h3><p>cuda流是每个设备的线性执行序列。不同设备间操作并行，而同一cuda流中操作按序</p>
<p>pytorch会自动的对默认流进行调度，但是对于用户自己创建的流就需要用户自行调度了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cuda = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">s = torch.cuda.Stream()  <span class="comment"># Create a new stream.</span></span><br><span class="line">A = torch.empty((<span class="number">100</span>, <span class="number">100</span>), device=cuda).normal_(<span class="number">0.0</span>, <span class="number">1.0</span>)</span><br><span class="line"><span class="keyword">with</span> torch.cuda.stream(s):</span><br><span class="line">    <span class="comment"># sum() may start execution before normal_() finishes!</span></span><br><span class="line">    B = torch.sum(A)</span><br></pre></td></tr></table></figure>
<h3 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h3><p>PyTorch使用缓存分配器来加速内存分配，无需设备同步即可快速释放内存。 但是，如果使用nvidia-smi，分配器管理的未使用内存仍将显示。可以使用empty_cache来释放这些未使用的缓存，但是这样并不增加Pytorch可用的GPU内存</p>
<h3 id="Best-practices"><a href="#Best-practices" class="headerlink" title="Best practices"></a>Best practices</h3><p>首先，确定是否使用GPU，一般使用cuda.is_available()判断，然后设置</p>
<p>在使用多GPU时，设置CUDA_VISIBLE_DEVICES来确定可用的GPU。手动设置默认GPU可以使用torch.cuda.device()</p>
<p>torch.Tensor.new_<em>类和 </em> _like类函数创建新tensor时会保存包括cuda属性在内的大部分属性，这是一种在前向传播过程中新建变量比较好的方法</p>
<p>源于pinned (page-locked) memory的数据从主机搬运到GPU上会更快，可以使用pin_memoy()生成cpu tensor得到副本</p>
<p>此外，一旦固定张量或存储，就可以使用异步GPU副本。只需将一个额外的non_blocking = True参数传递给cuda()调用即可。这可用于重叠进行数据传输与计算。</p>
<p>在创建DataLoader时可以设置pin_memory = True</p>
<p>可以使用nn.DataParallel 代替multiprocessing进行多进程操作，后者经常会出现很多问题</p>
<h2 id="EXTENDING-PYTORCH"><a href="#EXTENDING-PYTORCH" class="headerlink" title="EXTENDING PYTORCH"></a>EXTENDING PYTORCH</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/extending.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/extending.html</a></p>
<h3 id="扩展torch-autograd"><a href="#扩展torch-autograd" class="headerlink" title="扩展torch.autograd"></a>扩展torch.autograd</h3><p>tensor上的每一个操作都会创建一个新的function 对象，执行计算并记录进行的情况。计算的历史保存为DAG，顶点为function，边为计算关系（input &lt;- output），反向传播用拓扑排序的顺序来传递梯度</p>
<p>每一个function的object都只能在前向传播中使用一次</p>
<p>扩展torch.autograd等同于添加orch.autograd.Function的一个子类。任何torch.autograd的新函数都需要实现forward()和backward()</p>
<ul>
<li>forward：进行该function操作。可以输入任意多任何类型的参数，其中有些还可以作为optional的，第一个参数必须是用于保存上下文的ctx，在反向传播中被用于获得function的参数。记录历史的（即requires_grad = True）的Tensor参数将在调用之前不跟踪历史，且将会加入到计算图中。返回值可以是单个tensor或tensor的tuple</li>
<li>backward：对该function求导。接受上文的ctx作为第一个参数，接着跟随和forward返回的数量相同的参数。返回值数量应该和forward输入值相同。ctx可用于获取前向传播中保存的tensor参数。ctx也有一个tuple 变量ctx.needs_input_grad用于控制每一个input是否需要求梯度。每一传入的参数都是该节点关于给定output的梯度，而返回值应该是关于相应input的梯度，如果该input不是tensor类或者不需要求导，可以返回None</li>
</ul>
<p>torch.nn类中线性函数的实例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Inherit from Function</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearFunction</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Note that both forward and backward are @staticmethods</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="comment"># bias is an optional argument</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, input, weight, bias=None)</span>:</span></span><br><span class="line">        ctx.save_for_backward(input, weight, bias)</span><br><span class="line">        output = input.mm(weight.t())</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            output += bias.unsqueeze(<span class="number">0</span>).expand_as(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This function has only a single output, so it gets only one gradient</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="comment"># This is a pattern that is very convenient - at the top of backward</span></span><br><span class="line">        <span class="comment"># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span></span><br><span class="line">        <span class="comment"># None. Thanks to the fact that additional trailing Nones are</span></span><br><span class="line">        <span class="comment"># ignored, the return statement is simple even when the function has</span></span><br><span class="line">        <span class="comment"># optional inputs.</span></span><br><span class="line">        input, weight, bias = ctx.saved_tensors</span><br><span class="line">        grad_input = grad_weight = grad_bias = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># These needs_input_grad checks are optional and there only to</span></span><br><span class="line">        <span class="comment"># improve efficiency. If you want to make your code simpler, you can</span></span><br><span class="line">        <span class="comment"># skip them. Returning gradients for inputs that don't require it is</span></span><br><span class="line">        <span class="comment"># not an error.</span></span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">0</span>]:</span><br><span class="line">            grad_input = grad_output.mm(weight)</span><br><span class="line">        <span class="keyword">if</span> ctx.needs_input_grad[<span class="number">1</span>]:</span><br><span class="line">            grad_weight = grad_output.t().mm(input)</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> ctx.needs_input_grad[<span class="number">2</span>]:</span><br><span class="line">            grad_bias = grad_output.sum(<span class="number">0</span>).squeeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> grad_input, grad_weight, grad_bias</span><br></pre></td></tr></table></figure>
<p>为了利用该函数，将他的apply形式起别名:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear = LinearFunction.apply</span><br></pre></td></tr></table></figure>
<p>当然，也可以使用none-Tensor参数来传参：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MulConstant</span><span class="params">(Function)</span>:</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, tensor, constant)</span>:</span></span><br><span class="line">        <span class="comment"># ctx is a context object that can be used to stash information</span></span><br><span class="line">        <span class="comment"># for backward computation</span></span><br><span class="line">        ctx.constant = constant</span><br><span class="line">        <span class="keyword">return</span> tensor * constant</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        <span class="comment"># We return as many input gradients as there were arguments.</span></span><br><span class="line">        <span class="comment"># Gradients of non-Tensor arguments to forward must be None.</span></span><br><span class="line">        <span class="keyword">return</span> grad_output * ctx.constant, <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>backward的输入也可以是记录历史的tensor，因此可以在backward中实现高阶微分操作</p>
<p>如果要对自己编写的模块进行梯度校验，可以使用torch.autograd.gradcheck模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> gradcheck</span><br><span class="line"></span><br><span class="line"><span class="comment"># gradcheck takes a tuple of tensors as input, check if your gradient</span></span><br><span class="line"><span class="comment"># evaluated with these tensors are close enough to numerical</span></span><br><span class="line"><span class="comment"># approximations and returns True if they all verify this condition.</span></span><br><span class="line">input = (torch.randn(<span class="number">20</span>,<span class="number">20</span>,dtype=torch.double,requires_grad=<span class="literal">True</span>), torch.randn(<span class="number">30</span>,<span class="number">20</span>,dtype=torch.double,requires_grad=<span class="literal">True</span>))</span><br><span class="line">test = gradcheck(linear, input, eps=<span class="number">1e-6</span>, atol=<span class="number">1e-4</span>)</span><br><span class="line">print(test)</span><br></pre></td></tr></table></figure>
<h3 id="扩展torch-nn"><a href="#扩展torch-nn" class="headerlink" title="扩展torch.nn"></a>扩展torch.nn</h3><p>nn导出两种接口，modules versions and functional versions</p>
<h3 id="添加新的Module"><a href="#添加新的Module" class="headerlink" title="添加新的Module"></a>添加新的Module</h3><p>由于nn大量利用了autograd，因此添加新模块需要实现一个进行操作和求梯度的Function</p>
<p>要想将Function扩充为一个module，需要实现：</p>
<p><strong> init</strong>：可选，输入相关参数，初始化parameter和buffer</p>
<p>forward：实例化function并用它进行操作计算</p>
<blockquote>
<p>什么是buffer：</p>
<p><a href="https://zhuanlan.zhihu.com/p/89442276" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/89442276</a></p>
<p><a href="https://stackoverflow.com/questions/57540745/what-is-the-difference-between-register-parameter-and-register-buffer-in-pytorch" target="_blank" rel="noopener">https://stackoverflow.com/questions/57540745/what-is-the-difference-between-register-parameter-and-register-buffer-in-pytorch</a></p>
</blockquote>
<p>一个Linear模块的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_features, output_features, bias=True)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()</span><br><span class="line">        self.input_features = input_features</span><br><span class="line">        self.output_features = output_features</span><br><span class="line"></span><br><span class="line">        <span class="comment"># nn.Parameter is a special kind of Tensor, that will get</span></span><br><span class="line">        <span class="comment"># automatically registered as Module's parameter once it's assigned</span></span><br><span class="line">        <span class="comment"># as an attribute. Parameters and buffers need to be registered, or</span></span><br><span class="line">        <span class="comment"># they won't appear in .parameters() (doesn't apply to buffers), and</span></span><br><span class="line">        <span class="comment"># won't be converted when e.g. .cuda() is called. You can use</span></span><br><span class="line">        <span class="comment"># .register_buffer() to register buffers.</span></span><br><span class="line">        <span class="comment"># nn.Parameters require gradients by default.</span></span><br><span class="line">        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))</span><br><span class="line">        <span class="keyword">if</span> bias:</span><br><span class="line">            self.bias = nn.Parameter(torch.Tensor(output_features))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># You should always register all possible parameters, but the</span></span><br><span class="line">            <span class="comment"># optional ones can be None if you want.</span></span><br><span class="line">            self.register_parameter(<span class="string">'bias'</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Not a very smart way to initialize weights</span></span><br><span class="line">        self.weight.data.uniform_(<span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line">        <span class="keyword">if</span> bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.bias.data.uniform_(<span class="number">-0.1</span>, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="comment"># See the autograd section for explanation of what happens here.</span></span><br><span class="line">        <span class="keyword">return</span> LinearFunction.apply(input, self.weight, self.bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extra_repr</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># (Optional)Set the extra information about this module. You can test</span></span><br><span class="line">        <span class="comment"># it by printing an object of this class.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'in_features=&#123;&#125;, out_features=&#123;&#125;, bias=&#123;&#125;'</span>.format(</span><br><span class="line">            self.in_features, self.out_features, self.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h2 id="节约GPU的内存"><a href="#节约GPU的内存" class="headerlink" title="节约GPU的内存"></a>节约GPU的内存</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/faq.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/faq.html</a></p>
<h3 id="My-model-reports-“cuda-runtime-error-2-out-of-memory”"><a href="#My-model-reports-“cuda-runtime-error-2-out-of-memory”" class="headerlink" title="My model reports “cuda runtime error(2): out of memory”"></a>My model reports “cuda runtime error(2): out of memory”</h3><p>用完了GPU的内存，常见解决方法如下：</p>
<p>1.不要积累训练循环的历史</p>
<p>一般来说，包含记录梯度的tensor的计算都需要保存计算历史，不要让这样的tensor参与存在周期大于循环的计算中，比如累计计数。相反，应该使用detach或者直接访问基础数据</p>
<p>参考下面的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">total_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(input)</span><br><span class="line">    loss = criterion(output)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    total_loss += loss</span><br></pre></td></tr></table></figure>
<p>在这里，total_loss正在累积整个训练循环的历史记录，因为loss是一个保存求梯度历史的变量</p>
<p>要解决问题，只需要修改为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">total_loss += float(loss)</span><br></pre></td></tr></table></figure>
<p>2.不需要的tensor和变量赶快释放</p>
<p>对于Python，如果分配某个tensor给本地或者成员变量，直到超出这个变量的作用范围，才会释放最初的tensor</p>
<p>参考下面的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    intermediate = f(input[i])</span><br><span class="line">    result += g(intermediate)</span><br><span class="line">output = h(result)</span><br><span class="line"><span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>在这里，即使h正在执行，intermediate也会被保存，因为它的作用域超出了循环的结尾。为了显示释放空间，可以使用del intermediate</p>
<p>3.不要在太大的序列上跑RNN</p>
<p>4.不要使用太大的linear layers</p>
<h2 id="SERIALIZATION-SEMANTICS"><a href="#SERIALIZATION-SEMANTICS" class="headerlink" title="SERIALIZATION SEMANTICS"></a>SERIALIZATION SEMANTICS</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/serialization.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/serialization.html</a></p>
<h3 id="保存模型的方法"><a href="#保存模型的方法" class="headerlink" title="保存模型的方法"></a>保存模型的方法</h3><p>1.只保存模型参数（推荐）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.save(the_model.state_dict(), PATH)</span><br><span class="line"></span><br><span class="line">the_model = TheModelClass(*args, **kwargs)</span><br><span class="line">the_model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>
<p>2.保存整个模型</p>
<p>在这种情况下，序列化的数据将绑定到特定类和确切的目录，因此在其他project中使用时或重构后，可能会出现很多报错</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.save(the_model, PATH)</span><br><span class="line"></span><br><span class="line">the_model = torch.load(PATH)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a href="https://www.jianshu.com/p/60fc57e19615" target="_blank" rel="noopener">https://www.jianshu.com/p/60fc57e19615</a></p>
</blockquote>
<h2 id="多进程实例"><a href="#多进程实例" class="headerlink" title="多进程实例"></a>多进程实例</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/multiprocessing.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/multiprocessing.html</a></p>
<p>torch.multiprocessing用来替代Python本身的mutiprocessing</p>
<h2 id="设置随机数种子便于复现"><a href="#设置随机数种子便于复现" class="headerlink" title="设置随机数种子便于复现"></a>设置随机数种子便于复现</h2><p><a href="https://pytorch.org/docs/1.1.0/notes/randomness.html" target="_blank" rel="noopener">https://pytorch.org/docs/1.1.0/notes/randomness.html</a></p>
<p>网络的参数最初是随机的</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/14/LBS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="NaNillll">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NaNillll's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/07/14/LBS/" itemprop="url">LBS</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-14T12:03:37+08:00">
                2020-07-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  688
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="什么是Skinned-Mesh？"><a href="#什么是Skinned-Mesh？" class="headerlink" title="什么是Skinned Mesh？"></a>什么是Skinned Mesh？</h3><p>骨骼蒙皮。可以想象成整体的Mesh由很多子Mesh构成，每一个Mesh附着在Bone上，而Bone的方向和长度都由Joint确定，这样通过确定每一个Joint（Bone）就可以确定每一个子Mesh的坐标了。</p>
<h3 id="什么是LBS（Linear-Blending-Skinning）？"><a href="#什么是LBS（Linear-Blending-Skinning）？" class="headerlink" title="什么是LBS（Linear Blending Skinning）？"></a>什么是LBS（Linear Blending Skinning）？</h3><p>线性混合蒙皮。更进一步，每一个子Mesh（也就是Mesh的每一个Vertex）由多个Joint共同确定，赋给每一个Joint不同的权重，公式如下：</p>
<script type="math/tex; mode=display">
P’_i=\prod^m_{j=1}{w_{i,j}R_jP_i}</script><p>Pi表示初始位置时Vertex i在初始Joint j空间下的坐标，Rj表示此时刻world空间下到Joint j的变换，wij表示Joint j对Vertex i的影响权重，Pi‘表示此时Vertex i在world空间下的坐标。</p>
<p>上面的描述有些抽象，直观解释，就是最初某个vertex相对某个joint有偏移量d，所以移动后我们要在该joint此时的位置上加上偏移量d的变换值，作为此时该joint预测vertex应该在的位置</p>
<h3 id="如何记录和表达Joint和Bone？"><a href="#如何记录和表达Joint和Bone？" class="headerlink" title="如何记录和表达Joint和Bone？"></a>如何记录和表达Joint和Bone？</h3><p>LBS的简洁也是复杂之处在于，LBS包含Bone之间的父子关系，主要保存的是Joint之间的相对参数。</p>
<p>这样的简洁之处在于当祖Bone旋转或者平移时子Bone也会随之移动，其对应的Mesh也会随之移动，在处理时只需乘上一个变换矩阵即可。</p>
<p>但是复杂之处在于这也带来了许多恼人的空间变换。</p>
<h3 id="具体怎样计算？"><a href="#具体怎样计算？" class="headerlink" title="具体怎样计算？"></a>具体怎样计算？</h3><p><em>（我认为网上许多介绍此处的博客都忽略了Mesh或者Bone空间的时效性，也就是并没有说明清楚到底所谓的Bone空间到底在起初的位置，还是移动后的位置？<del>我觉得这一点还是比较影响理解的，也有可能是单纯的我太菜了</del>）</em></p>
<p>简要来说</p>
<script type="math/tex; mode=display">
V_{world} = V_{mesh} * Bone1OffsetMatrix * CombindMatrix1 * Weight</script><p> Vmesh 是顶点初始的坐标，一般取在world空间下（其实是无所谓的，下一步就会转换），Bone1OffsetMatrix1用来将初始时的顶点转换到初始时的Bone1空间，CombindMatrix1 用于将此时的Bone1空间转换为world空间。而初始的Bone1和此时的Bone1是同一个Bone，Mesh随Bone而动，因此最后的结果就是此时的Mesh。</p>
<p>在求CombindMatrix1 的过程中需要依靠父子Bone的层级关系，因为主要保存的是Joint之间的相对参数，一般需要不同层级间多个变换矩阵的连乘。</p>
<p>当然，只看公式还是理解的不透彻，具体找一个LBS的实例代码就好了<del>（比如SMPL）</del></p>
<blockquote>
<p>参考资料：</p>
<p><a href="https://www.cnblogs.com/shushen/p/5987280.html" target="_blank" rel="noopener">https://www.cnblogs.com/shushen/p/5987280.html</a></p>
<p><a href="https://blog.csdn.net/n5/article/details/3105872" target="_blank" rel="noopener">https://blog.csdn.net/n5/article/details/3105872</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/78377681" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/78377681</a></p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/13/SMPL-MANO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="NaNillll">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="NaNillll's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/07/13/SMPL-MANO/" itemprop="url">论文源码笔记SMPL_MANO</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-13T00:32:26+08:00">
                2020-07-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.7k
                </span>
              

              

              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="什么是SMPL"><a href="#什么是SMPL" class="headerlink" title="什么是SMPL"></a>什么是SMPL</h3><p>SMPL论文地址：<a href="https://smpl.is.tue.mpg.de/" target="_blank" rel="noopener">https://smpl.is.tue.mpg.de/</a></p>
<p>总而言之，SMPL就是通过LBS技术生成人体模型的算法，作者已经训练好了很多参数，使用时只需要输入shape参数beta和pose参数theta即可</p>
<p>LBS是SMPL模型的前提</p>
<p>既然是LBS，就需要Joint，SMPL共有K=23个Joint，可以生成N=6890个 vertices</p>
<h3 id="SMPL的计算过程"><a href="#SMPL的计算过程" class="headerlink" title="SMPL的计算过程"></a>SMPL的计算过程</h3><p>核心公式</p>
<img src="/2020/07/13/SMPL-MANO/1.PNG" class>
<p>基本和LBS相同。w表示权重，G’函数表示变换，剩下部分为静止的template，只不过这个template是一个关于theta和beta的函数。为了简便，实际上每个vertex由4个Joint确定</p>
<img src="/2020/07/13/SMPL-MANO/2.PNG" class>
<p>首先是G函数，G函数表示在当前的theta和J（即为各个joint坐标）下，转换到第k个joint空间下的转换矩阵是什么。A表示k的各个祖先joint的有序编号。很显然这是许多个齐次变换矩阵的乘积，即CombindMatrix。G’后半部分即表示如何把初始情况下的姿态转换到第k个joint空间,即为BoneOffsetMatrix</p>
<img src="/2020/07/13/SMPL-MANO/3.PNG" class>
<img src="/2020/07/13/SMPL-MANO/4.PNG" class>
<p>BS和BP函数均代表Rest Mesh相对template的形变，只不过一个是由于pose引起的，一个是由于shape本身引起的。BS叫Shape Blend Shape，BP叫Pose Blend Shape。Bp中的R函数实质上就是将轴角换成3x3旋转矩阵的指数运算，再将其拉伸为直向量，也是Rodrigues formula</p>
<p>BP和BS函数的计算过程中都可以使用PCA，简化运算</p>
<img src="/2020/07/13/SMPL-MANO/5.PNG" class>
<p>最后一个函数，J函数通过rest vertex预测rest joint，显然关节之间距离与shape是相关的。J是一个regression matrix </p>
<p>具体每一个函数的作用可以从下图中直观的看出来</p>
<img src="/2020/07/13/SMPL-MANO/6.PNG" class>
<h3 id="什么是MANO？"><a href="#什么是MANO？" class="headerlink" title="什么是MANO？"></a>什么是MANO？</h3><p>MANO论文地址：<a href="https://mano.is.tue.mpg.de/" target="_blank" rel="noopener">https://mano.is.tue.mpg.de/</a></p>
<p>基于SMPL的一个手部建模。Joint参数共K=15个，Vertex参数N=778，除此之外附加一个Global orientation。Shape参数的数量与选择的PCA有关</p>
<h3 id="MANO的代码分析"><a href="#MANO的代码分析" class="headerlink" title="MANO的代码分析"></a>MANO的代码分析</h3><p>项目地址：<a href="https://github.com/hassony2/manopth" target="_blank" rel="noopener">https://github.com/hassony2/manopth</a></p>
<p>核心是ManoLayer模块</p>
<p>只看论文很多细节都会忽略，而且论文和代码有一些不同之处</p>
<p>代码为基于Pytorch的MANO layer，只需要输入beta和Joint参数即可输出所有的Vertex和Joint坐标</p>
<h4 id="init函数"><a href="#init函数" class="headerlink" title="init函数"></a>init函数</h4><p>center_idx表示Joint中心的序号（只在最后输出时有用）</p>
<p>代码中的pose_coeffs并非论文所指的Pose参数，最终的Pose参数需要将mean hand对应的参数和pose_coeffs与主成分计算出的参数求和得到，主成分数量默认为6。flat_hand_mean表示mean hand对应的为flat hand，此时的Pose参数全为0；否则选择实验得到的mean pose对应的参数。beta长度为10，对应10个主成分</p>
<h4 id="forward函数"><a href="#forward函数" class="headerlink" title="forward函数"></a>forward函数</h4><p>以下主要分析基于轴角且使用PCA的情况</p>
<h5 id="生成rest-pose"><a href="#生成rest-pose" class="headerlink" title="生成rest pose"></a>生成rest pose</h5><p>th_full_pose将Global轴角和Pose参数连接在一起，长度为{batch_size，3*16}</p>
<p>th_posemap_axisang（）将轴角参数即th_full_pose转换为旋转矩阵，th_rot_map为直接的转换结果，长度为{batchsize，16*9}，th_pose_map为这个函数减去单位矩阵，因为rest pose时的pose参数为全零，对应的Rodrigues矩阵也为单位矩阵，因此对应了</p>
<img src="/2020/07/13/SMPL-MANO/4.PNG" class>
<p>之后调整th_rot_map，th_pose_map等维数除去Global transform</p>
<p>th_v_shaped即BS函数加上template，表示shape调控后的Mesh坐标，长度{batch_size，778，3}。share_betas表示是使用原始的beta还是全部使用beta的平均值</p>
<p>th_j即J函数，为th_v_shaped乘以regressor matrix，长度{batch_size，16，3}</p>
<p>th_v_posed为th_v_shaped加上BS函数，此时的th_v_posed即为骨骼变换前的最终静止Mesh，长度{batch_size，778，3}</p>
<h5 id="生成transformation"><a href="#生成transformation" class="headerlink" title="生成transformation"></a>生成transformation</h5><p>之后就是骨骼变换。模型的骨骼结构很简单，基本就是root joint和5个手指，每个手指有3个Joint，即三个层次空间，Joint间形成父子结构，这样恰好共1+5*3=16个Joint</p>
<p>th_with_zeros用于生成齐次变换矩阵，输入为旋转矩阵和平移向量</p>
<p>lev2_rel_transform_flt为上一级到2级空间的变换矩阵，lev2_flt表示world空间下到2级空间的变换矩阵，是由lev1_flt与lev2_rel_transform_flt相乘得到的，这即为一个层级变换过程。通过相似过程可以获得world空间下到各个Joint对应的变换矩阵</p>
<p>之后经过reorder_idxs的重排序，th_results和th_results_global即为最终到各个Joint空间的变换矩阵，即为Gk函数</p>
<p>之后，根据LBS的一贯步骤，需要乘以将初始Mesh转换到初始Bone位置的变换矩阵，论文中并没有直接乘以这个矩阵，因为初始Mesh到Bone只需要平移，因此只使用了一个矩阵加减，最终的结果为th_results2,具体过程如下</p>
<p>初始位置下Mesh到Bone的Offset矩阵：</p>
<script type="math/tex; mode=display">
Offset=\left[ \begin{matrix}   1 & -RestJoint \\   0 & 1   \end{matrix} \right]</script><p>而依据定义：</p>
<script type="math/tex; mode=display">
thResults=
\left[
 \begin{matrix}
   R_{trans} & V_{trans} \\
   0 & 1 
  \end{matrix} 
\right]</script><p>因此</p>
<script type="math/tex; mode=display">
thResult*Offset=Offset=\left[ \begin{matrix}   R_{trans} & -R_{trans}RestJoint+V_{trans} \\   0 & 1   \end{matrix} \right]</script><p>而代码中，则是定义了</p>
<script type="math/tex; mode=display">
jointJs=
\left[
 \begin{matrix}
   RestJoint  \\
   0 
  \end{matrix} 
\right]，
tmp2=thResults*jointJs=
\left[
 \begin{matrix}
   R_{trans}RestJoint  \\
   0 
  \end{matrix} 
\right]</script><script type="math/tex; mode=display">
thResults2=thResults-

\left[
 \begin{matrix}
   0& tmp2
  \end{matrix} 
\right]
=
\left[
 \begin{matrix}
   R_{transe}& V_{transe}-R_{trans}RestJoint   \\
   0& 1 
  \end{matrix} 
\right]</script><p>可以看出，th_results2就是最终的变换矩阵，也即论文中的G’函数，th_results2长度{batch size，16,4,4}</p>
<h5 id="生成最终结果"><a href="#生成最终结果" class="headerlink" title="生成最终结果"></a>生成最终结果</h5><p>之后，将th_results2与混合权重矩阵th_weights相乘，得到th_T，th_weights长度为{778,16}，th_T长度为{batch size，4, 4,778}</p>
<p>将骨骼变换前的最终静止Mesh即th_v_posed转换为齐次坐标th_rest_shape_h，长度{batch size，4,778}，并补上维度，化为{batch size，1，4,778}</p>
<p>最后，混合蒙皮，将th_T点乘th_rest_shape_h，并沿第三维求和，结果为th_verts，长度{batch size，778,4}即为最终结果。</p>
<p>这几步的计算顺序与论文中描述的是不同的，其中最让人费解的可能是最后一步：为什么选用th_T点乘th_rest_shape_h并沿第三维求和？第三维的维数为4，并非核心公式的K=16</p>
<p>实质上，此处的求和并非核心公式中混合各骨骼预测值的过程，各骨骼选择混合的过程已经在生成th_T时经历过了，这也是为什么th_T的维度为{batch size，4，4，778}，本质上就是778个矩阵，对应了778个Rest Joint，只不过这里是变换矩阵的转置，因此之后使用了点乘求和的形式</p>
<p>对于矩阵A，向量B，有：</p>
<script type="math/tex; mode=display">
AB=（A*B^T）.sum(1)</script><p>到现在，已经生成了目标Mesh的所有vertex</p>
<p>之后是生成常见的21个手部关节点，从th_results_global选出joint，在16个Joint的基础上加上tip并重新排序</p>
<p>可以选择palm（中指和手腕的中心）作为模型的中心来替代手腕，也可以选择其他的Joint</p>
<p>之后可以进行预制的移动</p>
<p>最后乘以1000转换为<em>milimeters</em></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">NaNillll</p>
              <p class="site-description motion-element" itemprop="description">记忆是有限的，外部记忆体是无限的</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/NaNillll" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">NaNillll</span>
  
 
  
  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">5.9k</span>
  
</div>











        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
